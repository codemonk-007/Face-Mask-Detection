{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from models import Darknet\n",
    "from utils.utils import load_classes, weights_init_normal\n",
    "from utils.datasets import ListDataset\n",
    "from utils.parse_config import parse_data_config\n",
    "from validate import evaluate\n",
    "\n",
    "from terminaltables import AsciiTable\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import tensorflow\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"number of epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=6, help=\"size of each image batch\")\n",
    "    parser.add_argument(\"--gradient_accumulations\", type=int, default=2, help=\"number of gradient accums before step\")\n",
    "    parser.add_argument(\"--model_def\", type=str, default=\"config/yolov3_mask.cfg\", help=\"path to model definition file\")\n",
    "    parser.add_argument(\"--data_config\", type=str, default=\"config/mask_dataset.data\", help=\"path to data config file\")\n",
    "    parser.add_argument(\"--pretrained_weights\", type=str, default=\"weights/yolov3.weights\", help=\"if specified starts from checkpoint model\")\n",
    "    parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "    parser.add_argument(\"--img_size\", type=int, default=416, help=\"size of each image dimension\")\n",
    "    parser.add_argument(\"--checkpoint_interval\", type=int, default=1, help=\"interval between saving model weights\")\n",
    "    parser.add_argument(\"--evaluation_interval\", type=int, default=1, help=\"interval evaluations on validation set\")\n",
    "    parser.add_argument(\"--compute_map\", default=True, help=\"if True computes mAP every tenth batch\")\n",
    "    parser.add_argument(\"--multiscale_training\", default=True, help=\"allow for multi-scale training\")\n",
    "    opt = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "data_config = parse_data_config(opt.data_config)\n",
    "train_path = data_config[\"train\"]\n",
    "valid_path = data_config[\"valid\"]\n",
    "class_names = load_classes(data_config[\"names\"])\n",
    "model = Darknet(opt.model_def).to(device)\n",
    "model.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.pretrained_weights:\n",
    "    if opt.pretrained_weights.endswith(\".pth\"):\n",
    "        model.load_state_dict(torch.load(opt.pretrained_weights))\n",
    "    else:\n",
    "        model.load_darknet_weights(opt.pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ListDataset(train_path, augment=True, multiscale=opt.multiscale_training)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=opt.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=opt.n_cpu,\n",
    "        pin_memory=True,\n",
    "        collate_fn=dataset.collate_fn,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "to_get_mAP = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for epoch in range(opt.epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        for batch_i, (_, imgs, targets) in enumerate(dataloader):\n",
    "            batches_done = len(dataloader) * epoch + batch_i\n",
    "\n",
    "            imgs = Variable(imgs.to(device))\n",
    "            targets =Variable(targets.to(device), requires_grad=False)\n",
    "\n",
    "            loss, outputs = model(imgs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            if batches_done % opt.gradient_accumulations:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            log_str = \"---- [Epoch %d/%d, Batch %d/%d] ----\" % (epoch, opt.epochs, batch_i, len(dataloader))\n",
    "            log_str += f\"Total loss {loss.item()}\"\n",
    "            print(log_str)\n",
    "\n",
    "            model.seen += imgs.size(0)\n",
    "\n",
    "        if epoch % opt.evaluation_interval == 0:\n",
    "            try:\n",
    "                precision, recall, AP, f1, ap_class = evaluate(\n",
    "                    model,\n",
    "                    path=valid_path,\n",
    "                    iou_thres=0.5,\n",
    "                    conf_thres=0.5,\n",
    "                    nms_thres=0.5,\n",
    "                    img_size=opt.img_size,\n",
    "                    batch_size=4,\n",
    "                )\n",
    "            except:\n",
    "                to_get_mAP = 999999999999\n",
    "\n",
    "        if epoch % opt.checkpoint_interval == 0:\n",
    "            torch.save(model.state_dict(), \"yolomodel-{}.h5\".format(epoch))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "290/290 [==============================] - ETA: 0s - loss: 0.1993 - acc: 0.9277INFO:tensorflow:Assets written to: model2-1.model/assets\n",
    "290/290 [==============================] - 100s 346ms/step - loss: 0.1993 - acc: 0.9277 - val_loss: 2.1043 - val_acc: 0.4864\n",
    "Epoch 2/10\n",
    "290/290 [==============================] - ETA: 0s - loss: 0.1939 - acc: 0.9298INFO:tensorflow:Assets written to: model2-2.model/assets\n",
    "290/290 [==============================] - 103s 355ms/step - loss: 0.1939 - acc: 0.9298 - val_loss: 1.9456 - val_acc: 0.5126\n",
    "Epoch 3/10\n",
    "290/290 [==============================] - ETA: 0s - loss: 0.1876 - acc: 0.9326INFO:tensorflow:Assets written to: model2-3.model/assets\n",
    "290/290 [==============================] - 127s 438ms/step - loss: 0.1876 - acc: 0.9326 - val_loss: 1.3266 - val_acc: 0.5889\n",
    "Epoch 4/10\n",
    "290/290 [==============================] - 110s 378ms/step - loss: 0.1951 - acc: 0.9367 - val_loss: 1.6606 - val_acc: 0.5407\n",
    "Epoch 5/10\n",
    "290/290 [==============================] - 109s 377ms/step - loss: 0.1800 - acc: 0.9367 - val_loss: 2.1061 - val_acc: 0.4613\n",
    "Epoch 6/10\n",
    "290/290 [==============================] - 102s 351ms/step - loss: 0.1767 - acc: 0.9409 - val_loss: 1.3883 - val_acc: 0.5477\n",
    "Epoch 7/10\n",
    "290/290 [==============================] - 106s 364ms/step - loss: 0.1852 - acc: 0.9336 - val_loss: 1.4504 - val_acc: 0.5146\n",
    "Epoch 8/10\n",
    "290/290 [==============================] - 108s 374ms/step - loss: 0.1784 - acc: 0.9371 - val_loss: 1.6237 - val_acc: 0.4342\n",
    "Epoch 9/10\n",
    "290/290 [==============================] - ETA: 0s - loss: 0.1710 - acc: 0.9388INFO:tensorflow:Assets written to: model2-9.model/assets\n",
    "290/290 [==============================] - 104s 360ms/step - loss: 0.1710 - acc: 0.9388 - val_loss: 1.1264 - val_acc: 0.6201\n",
    "Epoch 10/10\n",
    "290/290 [==============================] - 111s 382ms/step - loss: 0.1802 - acc: 0.9308 - val_loss: 1.9953 - val_acc: 0.4553"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
